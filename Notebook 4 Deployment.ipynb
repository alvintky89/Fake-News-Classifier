{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdd0579b-c6fb-4fd0-98ef-fbfe108ccefa",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# GA Capstone: Fake News Classifier\n",
    "\n",
    "Author: Tan Kai Yong Alvin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f00d14c-fee6-47b6-aa81-79f7042a0e11",
   "metadata": {},
   "source": [
    "# Notebook 4: Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db8dadff-9961-4f7b-9a90-cd071c212cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 2 additional cells in ML OPS 0 materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89bcb800-a9db-470b-b4d6-a47e9de213db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect this Jupyter notebook to the running MLFlow server\n",
    "import mlflow # import mlflow python package\n",
    "\n",
    "# save all experiments and runs in mlflow.db\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\") # set location of where mlflow is logging all the runs on your local computer, we're using a sqlite DB called mlflow.dbb\n",
    "# link inside set_tracking_uri will be replaced if using dagshub with what the platform provides as mlflow runs on their website\n",
    "# experiments will then be logged on dagshub cloud instead of on your local computer\n",
    "\n",
    "# Set the name of the experiment we're running in this notebook\n",
    "# MLFlow will connect to an existing experiment if the name passed already exists, \n",
    "# or create a new one if the experiment is not already present\n",
    "mlflow.set_experiment(\"Fake-News-Classifier\") # 1st time execution will yield warning \"experiment does not exist, creating new\". subsequent executions does not yield warning as experiment already exists & will be reused \n",
    "\n",
    "# refresh mlflow webpage if necessary to view this experiment name. future runs (say after shutting and reopening mlflow), will log under the same experiment name as long as the name isn't changed\n",
    "\n",
    "# Start automatically logging all runs below to the created MLFlow experiment\n",
    "mlflow.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6e1bb4-2cb6-4d44-ba19-30d0b3388506",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57b4f9ec-0a55-4605-8c28-6a8e42167060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import re\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline # to tag preprocessing transformers + estimators\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import get_scorer # we'll introduce this later\n",
    "from lightgbm import LGBMClassifier\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f70782c-933b-44e1-a36c-d05922d1041a",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9260c10a-ea78-47b7-8991-e9ee9d5ac158",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classification_df = pd.read_csv('./datasets/classification_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad965aab-c089-4bec-a909-61a49f7ba703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44689 entries, 0 to 44688\n",
      "Data columns (total 4 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   date                 44679 non-null  object\n",
      " 1   original_title_text  44689 non-null  object\n",
      " 2   classification_text  44680 non-null  object\n",
      " 3   label                44689 non-null  int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "classification_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c1a23eb-b447-45c2-8332-5a94cac55ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>original_title_text</th>\n",
       "      <th>classification_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>u budget fight loom republican flip fiscal scr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-29</td>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>u military accept transgender recruit monday p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>senior u republican senator let mr mueller job...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-30</td>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>fbi russia probe helped australian diplomat ti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-29</td>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>trump want postal service charge much amazon s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                original_title_text  \\\n",
       "0  2017-12-31  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  2017-12-29  U.S. military to accept transgender recruits o...   \n",
       "2  2017-12-31  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  2017-12-30  FBI Russia probe helped by Australian diplomat...   \n",
       "4  2017-12-29  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                 classification_text  label  \n",
       "0  u budget fight loom republican flip fiscal scr...      0  \n",
       "1  u military accept transgender recruit monday p...      0  \n",
       "2  senior u republican senator let mr mueller job...      0  \n",
       "3  fbi russia probe helped australian diplomat ti...      0  \n",
       "4  trump want postal service charge much amazon s...      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b91766dd-cd48-438f-8916-caa339be543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X and y\n",
    "X = classification_df['original_title_text']\n",
    "y = classification_df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3540d982-5ae5-4050-b8a4-6d582dc011ab",
   "metadata": {},
   "source": [
    "### Train/ Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1483186c-0876-48a0-bd52-9c90205015c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training and testing sets. Train/test split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb58166-8f23-459d-a6b8-297126363d0f",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01ca1a3b-b207-4b32-bbb5-1ca03c1bc6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words(\"english\")\n",
    "\n",
    "# some words provided the source of the article, which may reveal explicitly if the news is fake or real.Such words will be omitted to make the model less bias towards source\n",
    "add_stopwords = [\"21wire\", \"twitter\", \"reuters\", '21WIRE', '21st', 'Century',  'Wire', 'somodevilla', 'getty', 'images', 'subscribe', 'member', 'realdonaldtrump']\n",
    "stopwords_list.extend(add_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c5cabb9-20e9-4d8c-a9b2-53b03f6a6e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_cvec_lr = Pipeline([\n",
    "    ('cvec', CountVectorizer(ngram_range = (1,3), min_df = 0.001, max_features =3000, stop_words = 'english', token_pattern = '\\w+')), # this is the old change vs our previous GridSearch done with CountVectorizer()\n",
    "    ('lr', LogisticRegression(C=0.118, class_weight={}, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
    "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
    "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
    "                   warm_start=False))\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'cvec__max_features': [3000, 3500], \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4a42eae-1d2b-4120-9fef-c977e867a654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate GridSearchCV.\n",
    "gs_cvec_lr = GridSearchCV(pipe_cvec_lr, # the object that we are optimizing\n",
    "                  param_grid=params, \n",
    "                  cv=5) # 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f9d120f-ce7d-4253-b266-cc91dcafcfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/11/10 15:14:21 INFO mlflow.sklearn.utils: Logging the 5 best runs, no runs will be omitted.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    gs_cvec_lr.fit(X_train, y_train) # fit model on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3eb37ec-3bdc-471a-bc3b-e949936f47b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\alvintky89\\\\Documents\\\\GA\\\\my_materials\\\\12.01-mlops\\\\solution-code\\\\best_estimator'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlflow.artifacts import download_artifacts\n",
    "\n",
    "# Download the desired model from MLFlow to local directory\n",
    "# Get the URL by following instructions in above image (full path will be from 'model' folder instead for non-hyperparameter runs)\n",
    "full_path = './mlruns/1/573457d99490409c854f3433f9793e32/artifacts/best_estimator' # paste copied full path from best_estimator\n",
    "download_artifacts(full_path, dst_path='.') # download from source: full_path, destination path: where this solution code notebook is located (can reference with '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894768e6-ea79-4eb3-a0e0-1d560f38a5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "12.01-mlops/solution-code/best_estimator/model.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21ec41e9-4fe0-4057-87f0-b946985bf954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./best_estimator/model.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "filename = './best_estimator/model.pkl'\n",
    "joblib.dump(gs_cvec_lr, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76aa7bf-0edf-4b2f-85d2-42ab591fc51e",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4523c7d2-4f06-48bf-83f3-51e718ea5367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvintky89\\.conda\\envs\\dsi-sg-capstone\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.23.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\alvintky89\\.conda\\envs\\dsi-sg-capstone\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.23.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\alvintky89\\.conda\\envs\\dsi-sg-capstone\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator Pipeline from version 0.23.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\alvintky89\\.conda\\envs\\dsi-sg-capstone\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator GridSearchCV from version 0.23.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "import joblib\n",
    "model_classify = joblib.load(\"./best_estimator/model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f936e9de-76df-44a9-bfb2-e4875ca257a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = {\"text\":\"Schumer calls on Trump to appoint official to oversee Puerto Rico relief WASHINGTON (Reuters) - Charles Schumer, the top Democrat in the U.S. Senate, called on President Donald Trump on Sunday to name a single official to oversee and coordinate relief efforts in hurricane-ravaged Puerto Rico. Schumer, along with Representatives Nydia VelÃ zquez and Jose Serrano, said a â€œCEO of response and recoveryâ€ is needed to manage the complex and ongoing federal response in the territory, where millions of Americans remain without power and supplies. In a statement, Schumer said the current federal response to Hurricane Mariaâ€™s impact on the island had been â€œdisorganized, slow-footed and mismanaged.â€ â€œThis person will have the ability to bring all the federal agencies together, cut red tape on the public and private side, help turn the lights back on, get clean water flowing and help bring about recovery for millions of Americans who have gone too long in some of the worst conditions,â€ he said. The White House did not immediately respond to a request for comment. The Democrats contended that naming a lone individual to manage the governmentâ€™s relief efforts was critical, particularly given that the Federal Emergency Management Agency is already stretched thin from dealing with other crises, such as the aftermath of Hurricane Harvey in Texas and the wildfires in California. The severity of the Puerto Rico crisis, where a million people do not have clean water and millions are without power nearly a month after Hurricane Maria made landfall, demand a single person to focus exclusively on relief and recovery, the Democrats said. Forty-nine people have died in Puerto Rico officially, with dozens more missing. The hurricane did extensive damage to the islandâ€™s power grid, destroying homes, roads and other vital infrastructure. Now, the bankrupt territory is struggling to provide basic services like running water, and pay its bills. â€œItâ€™s tragically clear this Administration was caught flat footed when Maria hit Puerto Rico,â€ said VelÃ zquez. â€œAppointing a CEO of Response and Recovery will, at last, put one person with authority in charge to manage the response and ensure we are finally getting the people of Puerto Rico the aid they need.â€ On Thursday, Trump said the federal response has been a â€œ10â€ on a scale of one to 10 at a meeting with Puerto Rico Governor Ricardo Rossello.  The governor has asked the White House and Congress for at least $4.6 billion in block grants and other types of funding. Senator Marco Rubio called on Congress to modify an $18.7 billion aid package for areas damaged by a recent swath of hurricanes to ensure that Puerto Rico can quickly access the funds. \"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa58e2a2-d89e-4eaa-b1e9-03c83214639e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_classify.predict(pd.Series(user_input['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf77c9b-6a50-4d8a-a57f-10ef51742594",
   "metadata": {},
   "source": [
    "## Model Deployment with Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38fd12c6-7713-4bd6-a529-beb178755541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference.py \n",
    "from flask import Flask, request # import flask class, request module (to accept user inputs)\n",
    "import pandas as pd # to work with dataframe\n",
    "import os # to get port number that we'll hard-code to 8080  for now - important for deployment on Google Cloud (port no. will not be hard-coded there and sends a variable called port!)\n",
    "import mlflow.pyfunc # to load downloaded model for making predictions (same as mlops-1 'Making Predictions using the local downloaded model')\n",
    "import joblib\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "# Step 2: Instantiate the Flask API with name 'ModelEndpoint' ('api' is an object of the Flask() class)\n",
    "api = Flask('ModelEndpoint') # 'ModelEndpoint' can be called anything else as well\n",
    "\n",
    "# Step 3: Load the model from best_estimator folder for subsequently making predictions\n",
    "# model = mlflow.pyfunc.load_model(model_uri=\"./best_estimator\") # same code as mlops-1 'Making Predictions using the local downloaded model')\n",
    "# doing this as the above line does not work\n",
    "model_classify = joblib.load(\"./best_estimator/model.pkl\")\n",
    "\n",
    "# Step 4: Create the routes (we can create multiple! similar to multiple functions in a Python script)\n",
    "# Note: we'll need to name each route differently, similar to naming individual functions differently in a Python script\n",
    "\n",
    "## route 1: Health check. Just return success if the API is running\n",
    "@api.route('/') # this is a decorator (@api - using the Flask class' object instantiated above and creating a 'route' on this 'api' flask object. then we pass a name for this route as '/' just means home page)\n",
    "def home(): # just a normal Python function called 'home' with a decorator addition above\n",
    "    # return a simple string as JSON (JSON is just Python dictionary)\n",
    "    return {\"message\": \"Hi there!\", \"success\": True}, 200\n",
    "# returns dictionary with a message, code for success 'True' and html code 200 (https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "# a user going to the home page of this API, will see this message defined in the dictionary (we'll see this in action soon!)\n",
    "\n",
    "## route 2: accept input data, convert from JSON to dataframe, run predictions on the df, convert predictions to a list & return as dictionary. flask will take care of conversion to JSON object\n",
    "# 'POST' method is used when we want to receive some data from the user and POST it to the API. when we want to access the route '/predict' route, we'll always need to post some data to it, else it'll error\n",
    "@api.route('/predict', methods = ['POST']) # naming this 2nd route as /predict. so, in https://en.wikipedia.org/wiki, this is equivalent of the '/wiki', while the home page is what comes before '/wiki'\n",
    "def make_predictions(): # create a normal Python function for predictions\n",
    "    # step 1: Get the JSON object input data sent over the API\n",
    "    user_input = request.get_json(force=True) # use 'request' module (imported from Flask earlier)'s .get_json method\n",
    "    # by setting force=True in request.get_json, Flask will auto route input data sent to '/predict' route onto variable 'user_input'\n",
    "    import sys\n",
    "    # print(\"***********************\")\n",
    "    # print(type(user_input), user_input, file = sys.stderr)\n",
    "    # step 2: Convert user inputs (JSON object from step#1) to pandas dataframe\n",
    "    df_schema = {\"article\":str} # To ensure the feature columns for modeling get the correct datatype of float, because when Pandas converts from JSON to df, it infers dtype of every col\n",
    "    user_input_df = pd.read_json(StringIO(user_input), lines=True, dtype=df_schema) # Convert JSONL to dataframe with additional argument of dtype of what we're expecting the API to handle so model predictions work fine\n",
    "    print(\"***********************\")\n",
    "    print(type(user_input_df), user_input_df, file = sys.stderr)\n",
    "    # step 3: Run predictions using the loaded 'model' on user_input_df and convert predictions output from numpy array to list\n",
    "    predictions = model_classify.predict(pd.Series(user_input_df[\"article\"][0])).tolist()\n",
    "    \n",
    "    if predictions[0]==1:\n",
    "        return{'prediction': f'This is a FAKE news'}\n",
    "    else:\n",
    "        return{'prediction': f'This is a REAL news'}\n",
    "    \n",
    "    #return {'predictions': predictions} # return output of 'predict' route as a dictionary for Flask to convert to JSON object & send back to user at the '/predict' route. dictionary's key (can be any name) as 'predictions', values as list of model predictions\n",
    "    \n",
    "\n",
    "# Step 5: Main function that actually runs the API! - simply (blindly) copy+paste for all API runs\n",
    "if __name__ == '__main__': # good practise to have this main block whenever creating a .py file\n",
    "    api.run(host='0.0.0.0', # run the 'api' object created above with 2 routes on local host url '0.0.0.0' to just run on this computer\n",
    "            debug=True, # Debug=True ensures any changes to inference.py (like adding an extra print somewhere in this script) automatically updates the running API\n",
    "            port=int(os.environ.get(\"PORT\", 8080)) # just use 8080 by default\n",
    "           ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c420080-e5c3-4d11-a0a5-62ac76395d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference.py \n",
    "from flask import Flask, request # import flask class, request module (to accept user inputs)\n",
    "import pandas as pd # to work with dataframe\n",
    "import os # to get port number that we'll hard-code to 8080  for now - important for deployment on Google Cloud (port no. will not be hard-coded there and sends a variable called port!)\n",
    "import mlflow.pyfunc # to load downloaded model for making predictions (same as mlops-1 'Making Predictions using the local downloaded model')\n",
    "import joblib\n",
    "from io import StringIO\n",
    "import newspaper\n",
    "\n",
    "# Step 2: Instantiate the Flask API with name 'ModelEndpoint' ('api' is an object of the Flask() class)\n",
    "api = Flask('ModelEndpoint') # 'ModelEndpoint' can be called anything else as well\n",
    "\n",
    "# Step 3: Load the model from best_estimator folder for subsequently making predictions\n",
    "# model = mlflow.pyfunc.load_model(model_uri=\"./best_estimator\") # same code as mlops-1 'Making Predictions using the local downloaded model')\n",
    "# doing this as the above line does not work\n",
    "model_classify = joblib.load(\"./best_estimator/model.pkl\")\n",
    "\n",
    "# Step 4: Create the routes (we can create multiple! similar to multiple functions in a Python script)\n",
    "# Note: we'll need to name each route differently, similar to naming individual functions differently in a Python script\n",
    "\n",
    "## route 1: Health check. Just return success if the API is running\n",
    "@api.route('/') # this is a decorator (@api - using the Flask class' object instantiated above and creating a 'route' on this 'api' flask object. then we pass a name for this route as '/' just means home page)\n",
    "def home(): # just a normal Python function called 'home' with a decorator addition above\n",
    "    # return a simple string as JSON (JSON is just Python dictionary)\n",
    "    return {\"message\": \"Hi there!\", \"success\": True}, 200\n",
    "# returns dictionary with a message, code for success 'True' and html code 200 (https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "# a user going to the home page of this API, will see this message defined in the dictionary (we'll see this in action soon!)\n",
    "\n",
    "## route 2: accept input data, convert from JSON to dataframe, run predictions on the df, convert predictions to a list & return as dictionary. flask will take care of conversion to JSON object\n",
    "# 'POST' method is used when we want to receive some data from the user and POST it to the API. when we want to access the route '/predict' route, we'll always need to post some data to it, else it'll error\n",
    "@api.route('/predict', methods = ['POST']) # naming this 2nd route as /predict. so, in https://en.wikipedia.org/wiki, this is equivalent of the '/wiki', while the home page is what comes before '/wiki'\n",
    "def make_predictions(): # create a normal Python function for predictions\n",
    "    # step 1: Get the JSON object input data sent over the API\n",
    "    user_input = request.get_json(force=True) # use 'request' module (imported from Flask earlier)'s .get_json method\n",
    "    # by setting force=True in request.get_json, Flask will auto route input data sent to '/predict' route onto variable 'user_input'\n",
    "    import sys\n",
    "    # print(\"***********************\")\n",
    "    # print(type(user_input), user_input, file = sys.stderr)\n",
    "    # step 2: Convert user inputs (JSON object from step#1) to pandas dataframe\n",
    "    df_schema = {\"article\":str} # To ensure the feature columns for modeling get the correct datatype of float, because when Pandas converts from JSON to df, it infers dtype of every col\n",
    "    user_input_df = pd.read_json(StringIO(user_input), lines=True, dtype=df_schema) # Convert JSONL to dataframe with additional argument of dtype of what we're expecting the API to handle so model predictions work fine\n",
    "    print(\"***********************\")\n",
    "    print(type(user_input_df), user_input_df, file = sys.stderr)\n",
    "    # step 3: Run predictions using the loaded 'model' on user_input_df and convert predictions output from numpy array to list\n",
    "    predictions = model_classify.predict(pd.Series(user_input_df[\"article\"][0])).tolist()\n",
    "    \n",
    "    if predictions[0]==1:\n",
    "        return{'prediction': f'This is a FAKE news'}\n",
    "    else:\n",
    "        return{'prediction': f'This is a REAL news'}\n",
    "    \n",
    "    #return {'predictions': predictions} # return output of 'predict' route as a dictionary for Flask to convert to JSON object & send back to user at the '/predict' route. dictionary's key (can be any name) as 'predictions', values as list of model predictions\n",
    "    \n",
    "\n",
    "# Step 5: Main function that actually runs the API! - simply (blindly) copy+paste for all API runs\n",
    "if __name__ == '__main__': # good practise to have this main block whenever creating a .py file\n",
    "    api.run(host='0.0.0.0', # run the 'api' object created above with 2 routes on local host url '0.0.0.0' to just run on this computer\n",
    "            debug=True, # Debug=True ensures any changes to inference.py (like adding an extra print somewhere in this script) automatically updates the running API\n",
    "            port=int(os.environ.get(\"PORT\", 8080)) # just use 8080 by default\n",
    "           ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "004f8dff-9796-4c6c-b749-ccf15a52c14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# real news Senior U.S. Republican senator: 'Let Mr. Mueller do his job' WASHINGTON (Reuters) - The special counsel investigation of links between Russia and President Trumpâ€™s 2016 election campaign should continue without interference in 2018, despite calls from some Trump administration allies and Republican lawmakers to shut it down, a prominent Republican senator said on Sunday. Lindsey Graham, who serves on the Senate armed forces and judiciary committees, said Department of Justice Special Counsel Robert Mueller needs to carry on with his Russia investigation without political interference. â€œThis investigation will go forward. It will be an investigation conducted without political influence,â€ Graham said on CBSâ€™s Face the Nation news program. â€œAnd we all need to let Mr. Mueller do his job. I think heâ€™s the right guy at the right time.â€  The question of how Russia may have interfered in the election, and how Trumpâ€™s campaign may have had links with or co-ordinated any such effort, has loomed over the White House since Trump took office in January. It shows no sign of receding as Trump prepares for his second year in power, despite intensified rhetoric from some Trump allies in recent weeks accusing Muellerâ€™s team of bias against the Republican president. Trump himself seemed to undercut his supporters in an interview last week with the New York Times in which he said he expected Mueller was â€œgoing to be fair.â€    Russiaâ€™s role in the election and the question of possible links to the Trump campaign are the focus of multiple inquiries in Washington. Three committees of the Senate and the House of Representatives are investigating, as well as Mueller, whose team in May took over an earlier probe launched by the U.S. Federal Bureau of Investigation (FBI). Several members of the Trump campaign and administration have been convicted or indicted in the investigation.  Trump and his allies deny any collusion with Russia during the campaign, and the Kremlin has denied meddling in the election. Graham said he still wants an examination of the FBIâ€™s use of a dossier on links between Trump and Russia that was compiled by a former British spy, Christopher Steele, which prompted Trump allies and some Republicans to question Muellerâ€™s inquiry.   On Saturday, the New York Times reported that it was not that dossier that triggered an early FBI probe, but a tip from former Trump campaign foreign policy adviser George Papadopoulos to an Australian diplomat that Russia had damaging information about former Trump rival Hillary Clinton.  â€œI want somebody to look at the way the Department of Justice used this dossier. It bothers me greatly the way they used it, and I want somebody to look at it,â€ Graham said. But he said the Russia investigation must continue. â€œAs a matter of fact, it would hurt us if we ignored it,â€ he said. \n",
    "empty = {\"article\": \"Trump is dead\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "428551c5-4f06-4d52-b506-1c8e78ae8c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Prediction': 'This is a FAKE news'}\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "\n",
    "api_url = 'http://localhost:8080' # specify the URL to access\n",
    "api_route = '/predict' # specify the `route` to access in the URL\n",
    "\n",
    "# we'll need to use `requests.post()` based on our earlier specification in `\\predict` route to only accept a `POST` request \n",
    "response = requests.post(f'{api_url}{api_route}', json=json.dumps(empty))\n",
    "predictions = response.json()\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a03ba4d-3c3f-4832-8412-cd195f2115c9",
   "metadata": {},
   "source": [
    "## Create Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb5601a9-9d22-4500-ab81-14af63ac6d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting streamlit_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile streamlit_app.py\n",
    "import streamlit as st\n",
    "import requests\n",
    "import json\n",
    "from PIL import Image\n",
    "import base64\n",
    "\n",
    "def add_bg_from_local(image_file):\n",
    "    with open(image_file, \"rb\") as image_file:\n",
    "        encoded_string = base64.b64encode(image_file.read())\n",
    "    st.markdown(\n",
    "    f\"\"\"\n",
    "    <style>\n",
    "    .stApp {{\n",
    "        background-image: url(data:image/{\"png\"};base64,{encoded_string.decode()});\n",
    "        background-size: cover\n",
    "    }}\n",
    "    </style>\n",
    "    \"\"\",\n",
    "    unsafe_allow_html=True\n",
    "    )\n",
    "add_bg_from_local('background4.jpeg') \n",
    "\n",
    "# Title of the webpage\n",
    "st.title(\"Fake News Classifier\")\n",
    "\n",
    "st.subheader(\"\"\"Use this application to differentiate REAL and FAKE news.\"\"\")\n",
    "\n",
    "with st.form(key='myform', clear_on_submit = True):\n",
    "    article = st.text_input('Enter the text of the news')\n",
    "    submit = st.form_submit_button(\"Predict\")\n",
    "\n",
    "user_input = {'article': article}\n",
    "\n",
    "st.write(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe042ce0-6755-4d3d-8e36-a95c101205b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to streamlit_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a streamlit_app.py\n",
    "if submit:\n",
    "    with st.spinner('Reading the news...'):\n",
    "        api_url = 'http://localhost:8080' # specify the URL to access\n",
    "        api_route = '/predict' # specify the `route` to access in the URL\n",
    "        \n",
    "        # we'll need to use `requests.post()` based on our earlier specification in `\\predict` route to only accept a `POST` request \n",
    "        response = requests.post(f'{api_url}{api_route}', json=json.dumps(user_input))\n",
    "        predictions = response.json()\n",
    "        \n",
    "        st.success('Completed')\n",
    "        st.header('Verdict:')\n",
    "        st.write(predictions['prediction'])\n",
    "        #st.write(f\"Prediction: {predictions['predictions'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c40eadb-5526-428a-a0cf-4170a2b59d89",
   "metadata": {},
   "source": [
    "Instruction to run this in the local host:\n",
    "\n",
    "Activate the dsi-sg conda environment: `conda activate dsi-sg-capstone1` or `mamba activate dsi-sg` depending on your installation -> executing this changes the `base` environment to `dsi-sg` on your terminal window\n",
    "- Run the file as a normal python file by typing this in your terminal window and press enter: `python inference.py`\n",
    "\n",
    "Next,\n",
    "\n",
    "Ensure to do conda activate dsi-sg-capstone1 \n",
    "Run: streamlit run streamlit_app.py\n",
    "\n",
    "Local host link: http://localhost:8501/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
